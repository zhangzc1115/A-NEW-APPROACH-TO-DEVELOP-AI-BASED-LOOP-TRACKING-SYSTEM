{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "M71uJbSbjDwn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import tensorflow as tf\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzJR6j47lFKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data is not included due to NDA reasons. If there's request or need, please contact Dr. Peter Huang at (703) 589-4919\n",
        "f = open(\"data_truck.txt\",\"r\")\n",
        "\n",
        "t_dataX = []\n",
        "t_dataY = []\n",
        "for line in f:\n",
        "  line = line.split(',')\n",
        "  line.pop()\n",
        "  line = [float(i) for i in line[5:]]\n",
        "  \n",
        "  #some filters for data to delete the anomalies\n",
        "  if line[0] in (2,3,5):\n",
        "    if line[1] > 1:\n",
        "      if 200 < line[2] < 384:\n",
        "        line = [int(i) for i in line]\n",
        "        t_dataY.append(line[0])\n",
        "        t_dataX.append([line[1]]+line[3:])\n",
        "\n",
        "X = pd.DataFrame(t_dataX)\n",
        "X = X.fillna(4)\n",
        "Y = np.array(t_dataY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1ciGGlXn6Tj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reserve test data using stratified fold, but only 1 fold is actually used.\n",
        "\n",
        "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
        "\n",
        "cv.get_n_splits(X,Y)\n",
        "for train_index, test_index in cv.split(X, Y):\n",
        "  x_train, x_test = X.values[train_index], X.values[test_index]\n",
        "  y_train, y_test = Y[train_index], Y[test_index]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8tHhhULlOFd",
        "colab_type": "code",
        "outputId": "5699fb15-6755-4271-ea80-267aaf5da3c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#random forest classifier\n",
        "clf = RandomForestClassifier(n_estimators=200, max_depth=6, random_state=0)\n",
        "\n",
        "#cross validation\n",
        "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
        "scoring = ['accuracy']\n",
        "scores = cross_validate(clf, x_train, y_train, scoring=scoring,cv=cv)\n",
        "print('Mean accuracy for random forest: ', scores['test_accuracy'].mean())"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean accuracy for random forest:  0.9076547231270358\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKGirGGhmvrt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2ae60dd-84f0-47df-ab05-e27b6aa0608c"
      },
      "source": [
        "#adam solver is used in field data instead of lbfgs, since there is a large amount of data\n",
        "clf2 = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(30,), random_state=1)\n",
        "\n",
        "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
        "scoring = ['accuracy']\n",
        "scores = cross_validate(clf2, x_train, y_train, scoring=scoring,cv=cv)\n",
        "print('Mean accuracy for MLP classifier: ', scores['test_accuracy'].mean())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean accuracy for MLP classifier:  0.9120521172638437\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVSE8GGAo5F5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "b5b0c6af-32eb-4a6e-84a6-4622f5031f92"
      },
      "source": [
        "#dense neural network by tensorflow training\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(384,)),\n",
        "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dropout(0.05),\n",
        "  tf.keras.layers.Dense(6, activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=20)\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0730 16:08:32.256854 140352147982208 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "6137/6137 [==============================] - 2s 328us/sample - loss: 12.8822 - acc: 0.8305\n",
            "Epoch 2/20\n",
            "6137/6137 [==============================] - 2s 256us/sample - loss: 0.6746 - acc: 0.8841\n",
            "Epoch 3/20\n",
            "6137/6137 [==============================] - 2s 264us/sample - loss: 0.3143 - acc: 0.8941\n",
            "Epoch 4/20\n",
            "6137/6137 [==============================] - 2s 254us/sample - loss: 0.2719 - acc: 0.9092\n",
            "Epoch 5/20\n",
            "6137/6137 [==============================] - 2s 257us/sample - loss: 0.2395 - acc: 0.9136\n",
            "Epoch 6/20\n",
            "6137/6137 [==============================] - 2s 261us/sample - loss: 0.2462 - acc: 0.9112\n",
            "Epoch 7/20\n",
            "6137/6137 [==============================] - 2s 260us/sample - loss: 0.2384 - acc: 0.9159\n",
            "Epoch 8/20\n",
            "6137/6137 [==============================] - 2s 261us/sample - loss: 0.2283 - acc: 0.9158\n",
            "Epoch 9/20\n",
            "6137/6137 [==============================] - 2s 264us/sample - loss: 0.2267 - acc: 0.9138\n",
            "Epoch 10/20\n",
            "6137/6137 [==============================] - 2s 265us/sample - loss: 0.2285 - acc: 0.9184\n",
            "Epoch 11/20\n",
            "6137/6137 [==============================] - 2s 257us/sample - loss: 0.2109 - acc: 0.9192\n",
            "Epoch 12/20\n",
            "6137/6137 [==============================] - 2s 261us/sample - loss: 0.1949 - acc: 0.9263\n",
            "Epoch 13/20\n",
            "6137/6137 [==============================] - 2s 262us/sample - loss: 0.2096 - acc: 0.9193\n",
            "Epoch 14/20\n",
            "6137/6137 [==============================] - 2s 258us/sample - loss: 0.2000 - acc: 0.9215\n",
            "Epoch 15/20\n",
            "6137/6137 [==============================] - 2s 262us/sample - loss: 0.1981 - acc: 0.9257\n",
            "Epoch 16/20\n",
            "6137/6137 [==============================] - 2s 259us/sample - loss: 0.2027 - acc: 0.9229\n",
            "Epoch 17/20\n",
            "6137/6137 [==============================] - 2s 258us/sample - loss: 0.1980 - acc: 0.9219\n",
            "Epoch 18/20\n",
            "6137/6137 [==============================] - 2s 259us/sample - loss: 0.1953 - acc: 0.9247\n",
            "Epoch 19/20\n",
            "6137/6137 [==============================] - 2s 266us/sample - loss: 0.2193 - acc: 0.9175\n",
            "Epoch 20/20\n",
            "6137/6137 [==============================] - 2s 266us/sample - loss: 0.1923 - acc: 0.9216\n",
            "1535/1535 [==============================] - 0s 100us/sample - loss: 0.1721 - acc: 0.9381\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.17212067023162345, 0.93811077]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    }
  ]
}